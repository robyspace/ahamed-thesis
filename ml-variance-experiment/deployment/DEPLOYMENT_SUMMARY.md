# Hybrid ML Router - Deployment Summary

**Date:** 2025-11-24
**Status:** âœ… Successfully Deployed
**Region:** eu-west-1

---

## Overview

Successfully deployed an ML-based platform routing system for a hybrid serverless-container architecture. The system uses a Neural Network model to predict optimal platform placement (Lambda vs ECS) based on workload characteristics.

---

## Deployed Components

### 1. Lambda Function (Container Image)
- **Name:** `hybrid-ml-router`
- **Package Type:** Container Image (Docker)
- **Runtime:** Python 3.9
- **Memory:** 512 MB
- **Timeout:** 30 seconds
- **Architecture:** x86_64
- **ECR Repository:** `851974186570.dkr.ecr.eu-west-1.amazonaws.com/hybrid-ml-router:latest`

**Dependencies:**
- numpy==2.0.2
- scikit-learn==1.6.1
- scipy==1.13.1
- joblib==1.5.2

**Model:**
- Type: Neural Network (MLPClassifier)
- Test Accuracy: 69.58%
- Training Date: 2025-11-23
- Model Size: ~95 KB

### 2. API Gateway REST API
- **API ID:** `73jtubvdei`
- **Name:** `hybrid-ml-router-api`
- **Stage:** prod
- **Endpoint:** `https://73jtubvdei.execute-api.eu-west-1.amazonaws.com/prod/predict`
- **Method:** POST /predict
- **Integration:** Lambda Proxy

### 3. Monitoring
- **CloudWatch Logs:** `/aws/lambda/hybrid-ml-router`
- **Log Retention:** 7 days
- **Metrics:** Lambda invocations, duration, errors, throttles

---

## Performance Metrics

### Cold Start
- **First Invocation:** ~7.8 seconds
- **Reason:** Container initialization + model loading

### Warm Invocations
- **Average Latency:** 1-3 ms
- **Target Met:** âœ… Yes (target: <10ms)

### Cost per Request
- **Cold Start:** $0.000065
- **Warm Request:** $0.00000021
- **Extremely Cost-Effective:** âœ… Yes

---

## Test Results

### Test Case 1: Lightweight API
**Input:**
```json
{
  "workload_type": "lightweight_api",
  "payload_size_kb": 5.0,
  "time_window": "midday",
  "load_pattern": "medium_load"
}
```

**Output:**
- **Platform:** ECS
- **Confidence:** 57.6%
- **Inference Time:** 2.6 seconds (warm: 1ms)

### Test Case 2: Heavy Processing
**Input:**
```json
{
  "workload_type": "heavy_processing",
  "payload_size_kb": 5120.0,
  "time_window": "evening",
  "load_pattern": "burst_load"
}
```

**Output:**
- **Platform:** Lambda
- **Confidence:** 100%
- **Inference Time:** 1ms

---

## API Usage

### Endpoint
```
POST https://73jtubvdei.execute-api.eu-west-1.amazonaws.com/prod/predict
```

### Example Request
```bash
curl -X POST https://73jtubvdei.execute-api.eu-west-1.amazonaws.com/prod/predict \
  -H "Content-Type: application/json" \
  -d '{
    "workload_type": "lightweight_api",
    "payload_size_kb": 5.0,
    "time_window": "midday",
    "load_pattern": "medium_load"
  }'
```

### Response Format
```json
{
  "prediction": {
    "platform": "ecs",
    "confidence": 0.5757885319319196,
    "lambda_probability": 0.42421146806808047,
    "ecs_probability": 0.5757885319319196
  },
  "metrics": {
    "inference_time_ms": 1.0128,
    "inference_cost_usd": 2.0844003458023072e-07,
    "timestamp": "2025-11-24T14:32:01.065218Z"
  },
  "request_context": {
    "workload_type": "lightweight_api",
    "payload_size_kb": 5.0,
    "time_window": "midday",
    "load_pattern": "medium_load"
  },
  "model_info": {
    "model_type": "Neural Network",
    "test_accuracy": 0.6958543639766347,
    "training_date": "2025-11-23T15:06:49.593301"
  }
}
```

---

## Architecture Decisions

### Why Container Image?
1. **Size Limitations:** scikit-learn + numpy + scipy exceeded Lambda's 250MB limit for layers
2. **Flexibility:** Container images support up to 10GB
3. **Easier Management:** Single artifact to manage
4. **Platform Compatibility:** Built for linux/amd64 from ARM Mac using Docker

### Why Neural Network?
1. **Accuracy:** 69.58% test accuracy on balanced dataset
2. **Performance:** Fast inference (<10ms target met)
3. **Size:** Compact model (~95KB) loads quickly

---

## Key Challenges Solved

### 1. Dependency Size Issue
- **Problem:** sklearn + numpy + scipy exceeded Lambda deployment limits
- **Solution:** Used container images instead of zip packages

### 2. Architecture Compatibility
- **Problem:** Building on ARM Mac (M4) for x86_64 Lambda
- **Solution:** Used `--platform linux/amd64` in Docker build

### 3. Library Version Mismatch
- **Problem:** Model pickled with numpy 2.0.2, initial deployment used 1.24.3
- **Solution:** Matched exact library versions used during training

### 4. Type Conversion Bug
- **Problem:** `context.memory_limit_in_mb` returned string instead of int
- **Solution:** Added `float()` conversion in cost calculation

---

## Monitoring Setup

### CloudWatch Metrics
Access metrics at: https://console.aws.amazon.com/cloudwatch/home?region=eu-west-1

**Key Metrics to Monitor:**
1. **Invocations:** Number of predictions
2. **Duration:** Inference latency
3. **Errors:** Failed predictions
4. **Throttles:** Rate limiting
5. **ConcurrentExecutions:** Scaling behavior

### Viewing Logs
```bash
aws logs tail /aws/lambda/hybrid-ml-router --follow
```

---

## Cost Analysis

### Lambda Inference Costs (512 MB, 1ms inference)
- **Request Cost:** $0.20 per 1M requests = $0.0000002 per request
- **Duration Cost:** 512MB Ã— 0.001s Ã— $0.0000166667/GB-sec = $0.0000000084
- **Total per request:** ~$0.00000021

### Projected Costs
- **1,000 requests:** $0.0002
- **1 million requests:** $0.21
- **Extremely cost-effective for routing decisions**

---

## Next Steps (Per Deployment Guide)

- âœ… 1. Deploy Lambda function
- âœ… 2. Test inference
- âœ… 3. Set up API Gateway
- âœ… 4. Configure monitoring
- ðŸ”„ 5. Integrate with workload router
- ðŸ”„ 6. Run comparative evaluation (Lambda-only vs ECS-only vs ML-hybrid)
- ðŸ”„ 7. Measure cost and latency savings
- ðŸ”„ 8. Document results for thesis

---

## Files and Resources

### Deployment Files
- `Dockerfile` - Container image definition
- `requirements.txt` - Python dependencies
- `lambda_handler.py` - Lambda function code
- `variance_model_best.pkl` - Trained ML model (95KB)
- `feature_columns.json` - Feature schema
- `model_metadata.json` - Model information

### AWS Resources
- Lambda Function: `hybrid-ml-router`
- ECR Repository: `hybrid-ml-router`
- API Gateway: `hybrid-ml-router-api`
- CloudWatch Log Group: `/aws/lambda/hybrid-ml-router`

---

## Security Configuration

### IAM Role
- Role: `lambda-execution-role`
- Permissions: Basic Lambda execution (CloudWatch Logs)

### API Gateway
- **Authentication:** None (open for testing)
- **âš ï¸ Recommendation:** Add API key or IAM authorization for production

### Network
- **Endpoint Type:** Regional
- **CORS:** Enabled via Lambda response headers

---

## Maintenance

### Updating the Model
1. Retrain model with new data
2. Export new `variance_model_best.pkl`
3. Rebuild Docker image
4. Push to ECR
5. Update Lambda function

### Updating Dependencies
1. Modify `requirements.txt`
2. Rebuild Docker image
3. Push to ECR
4. Update Lambda function

### Commands
```bash
# Rebuild and deploy
cd deployment/lambda-inference
docker build --platform linux/amd64 --provenance=false --sbom=false \
  -t 851974186570.dkr.ecr.eu-west-1.amazonaws.com/hybrid-ml-router:latest .
docker push 851974186570.dkr.ecr.eu-west-1.amazonaws.com/hybrid-ml-router:latest
aws lambda update-function-code --function-name hybrid-ml-router \
  --image-uri 851974186570.dkr.ecr.eu-west-1.amazonaws.com/hybrid-ml-router:latest
```

---

## Success Criteria

- âœ… Inference latency < 10ms (achieved: ~1ms warm)
- âœ… Model accuracy > 65% (achieved: 69.58%)
- âœ… Cost per inference < $0.001 (achieved: $0.00000021)
- âœ… API Gateway successfully integrated
- âœ… Monitoring configured

---

## Contact & Support

For issues or questions:
1. Check CloudWatch Logs
2. Review this documentation
3. Consult AWS_DEPLOYMENT_GUIDE.md

---

**Deployment Status:** âœ… Production Ready
**Last Updated:** 2025-11-24
**Version:** 1.0
