{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hybrid Serverless-Container ML Model Training\n",
    "## Variance-Aware Decision Boundary Learning\n",
    "\n",
    "**Objective:** Train ML models to predict optimal platform (Lambda vs ECS) based on context\n",
    "\n",
    "**Key Challenge:** Previous models achieved 100% accuracy by memorizing workload_type ‚Üí label\n",
    "\n",
    "**Solution:** Variance-aware features force model to learn real decision boundaries\n",
    "\n",
    "**Target Accuracy:** 75-90% (NOT 100%!)\n",
    "\n",
    "---\n",
    "\n",
    "### Notebook Structure:\n",
    "1. Environment Setup & Data Loading\n",
    "2. Exploratory Data Analysis\n",
    "3. Model Training (4 models)\n",
    "4. Model Evaluation & Comparison\n",
    "5. Feature Importance Analysis\n",
    "6. SHAP Analysis (Interpretability)\n",
    "7. Model Selection & Saving\n",
    "8. Inference Testing\n",
    "9. Deployment Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Environment Setup & Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (Google Colab)\n",
    "!pip install -q pandas numpy scikit-learn xgboost lightgbm matplotlib seaborn shap joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "# ML Libraries\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    confusion_matrix, classification_report, roc_auc_score, roc_curve\n",
    ")\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "import shap\n",
    "import joblib\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set plot style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"‚úÖ All libraries imported successfully!\")\n",
    "print(f\"Notebook run: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload Training Data to Colab\n",
    "\n",
    "**Option 1:** Upload manually\n",
    "```python\n",
    "from google.colab import files\n",
    "uploaded = files.upload()\n",
    "```\n",
    "\n",
    "**Option 2:** Mount Google Drive\n",
    "```python\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "```\n",
    "\n",
    "**Option 3:** Use the file from repository (update path below)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTION 1: Upload file manually (uncomment to use)\n",
    "# from google.colab import files\n",
    "# uploaded = files.upload()\n",
    "# DATA_FILE = list(uploaded.keys())[0]\n",
    "\n",
    "# OPTION 2: Use Google Drive (uncomment and update path)\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "# DATA_FILE = '/content/drive/MyDrive/ml_training_data_variance_v1_20251123_134539.csv'\n",
    "\n",
    "# OPTION 3: Direct path (update with your file)\n",
    "DATA_FILE = 'ml_training_data_variance_v1_20251123_134539.csv'\n",
    "\n",
    "print(f\"Data file: {DATA_FILE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "print(\"Loading training data...\")\n",
    "df = pd.read_csv(DATA_FILE)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DATA LOADED SUCCESSFULLY\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Total samples: {len(df):,}\")\n",
    "print(f\"Features: {len(df.columns)}\")\n",
    "print(f\"\\nColumns: {list(df.columns)}\")\n",
    "\n",
    "# Display first few rows\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Exploratory Data Analysis\n",
    "\n",
    "Understanding the variance in our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define feature columns (NO actual performance metrics!)\n",
    "FEATURE_COLUMNS = [\n",
    "    'workload_type_encoded',\n",
    "    'payload_size_kb',\n",
    "    'time_window_encoded',\n",
    "    'load_pattern_encoded',\n",
    "    'hour_of_day',\n",
    "    'is_weekend',\n",
    "    'lambda_memory_limit_mb',\n",
    "    'payload_squared',\n",
    "    'payload_log',\n",
    "    'payload_workload_interaction',\n",
    "    'payload_hour_interaction',\n",
    "    'payload_time_window_interaction',\n",
    "    'workload_time_window_interaction',\n",
    "    'payload_load_pattern_interaction',\n",
    "    'time_window_load_pattern_interaction'\n",
    "]\n",
    "\n",
    "TARGET_COLUMN = 'balanced_optimal'\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FEATURE VALIDATION\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nPredictive features: {len(FEATURE_COLUMNS)}\")\n",
    "for i, feat in enumerate(FEATURE_COLUMNS, 1):\n",
    "    print(f\"  {i:2d}. {feat}\")\n",
    "\n",
    "print(f\"\\nTarget variable: {TARGET_COLUMN}\")\n",
    "\n",
    "# Check for data leakage\n",
    "forbidden_features = ['lambda_latency_ms', 'ecs_latency_ms', 'lambda_cost_usd', 'ecs_cost_usd']\n",
    "leaked = [f for f in forbidden_features if f in df.columns]\n",
    "if leaked:\n",
    "    print(f\"\\n‚ùå DATA LEAKAGE DETECTED: {leaked}\")\n",
    "else:\n",
    "    print(\"\\n‚úÖ NO DATA LEAKAGE - Clean feature set!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label distribution analysis\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"LABEL DISTRIBUTION ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Overall distribution\n",
    "label_counts = df[TARGET_COLUMN].value_counts()\n",
    "print(f\"\\nOverall Distribution:\")\n",
    "print(f\"  Lambda optimal (1): {label_counts.get(1, 0):,} ({label_counts.get(1, 0)/len(df)*100:.1f}%)\")\n",
    "print(f\"  ECS optimal (0):    {label_counts.get(0, 0):,} ({label_counts.get(0, 0)/len(df)*100:.1f}%)\")\n",
    "\n",
    "# Distribution by workload type\n",
    "print(f\"\\nDistribution by Workload Type:\")\n",
    "workload_stats = df.groupby('workload_type')[TARGET_COLUMN].agg([\n",
    "    ('count', 'count'),\n",
    "    ('lambda_optimal', 'sum'),\n",
    "    ('lambda_pct', lambda x: x.mean() * 100)\n",
    "]).round(1)\n",
    "print(workload_stats)\n",
    "\n",
    "# Variance validation\n",
    "print(f\"\\nüéØ Variance Validation (Target: 20-80%):\")\n",
    "for workload, row in workload_stats.iterrows():\n",
    "    pct = row['lambda_pct']\n",
    "    if 20 <= pct <= 80:\n",
    "        status = \"‚úÖ GOOD\"\n",
    "    elif pct < 20 or pct > 80:\n",
    "        status = \"‚ö†Ô∏è  Borderline\"\n",
    "    else:\n",
    "        status = \"‚ùå ISSUE\"\n",
    "    print(f\"  {workload:25s} {pct:5.1f}% Lambda optimal  {status}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization: Label distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Overall distribution\n",
    "ax1 = axes[0]\n",
    "label_counts.plot(kind='bar', ax=ax1, color=['#e74c3c', '#3498db'])\n",
    "ax1.set_title('Overall Label Distribution', fontsize=14, fontweight='bold')\n",
    "ax1.set_xlabel('Label (0=ECS, 1=Lambda)', fontsize=12)\n",
    "ax1.set_ylabel('Count', fontsize=12)\n",
    "ax1.set_xticklabels(['ECS Optimal', 'Lambda Optimal'], rotation=0)\n",
    "for i, v in enumerate(label_counts):\n",
    "    ax1.text(i, v + 500, f'{v:,}\\n({v/len(df)*100:.1f}%)', ha='center', fontsize=11)\n",
    "\n",
    "# By workload type\n",
    "ax2 = axes[1]\n",
    "workload_pct = df.groupby('workload_type')[TARGET_COLUMN].mean() * 100\n",
    "workload_pct.plot(kind='barh', ax=ax2, color='#2ecc71')\n",
    "ax2.set_title('Lambda Optimal % by Workload', fontsize=14, fontweight='bold')\n",
    "ax2.set_xlabel('Lambda Optimal (%)', fontsize=12)\n",
    "ax2.set_ylabel('Workload Type', fontsize=12)\n",
    "ax2.axvline(x=50, color='red', linestyle='--', linewidth=2, alpha=0.7, label='50% threshold')\n",
    "ax2.axvspan(20, 80, alpha=0.1, color='green', label='Target variance range')\n",
    "ax2.legend()\n",
    "for i, v in enumerate(workload_pct):\n",
    "    ax2.text(v + 1, i, f'{v:.1f}%', va='center', fontsize=11)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úÖ Variance visualization complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature distributions\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FEATURE STATISTICS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Key features\n",
    "key_features = ['payload_size_kb', 'time_window_encoded', 'load_pattern_encoded', 'workload_type_encoded']\n",
    "print(f\"\\nKey Feature Statistics:\")\n",
    "print(df[key_features].describe().round(2))\n",
    "\n",
    "# Unique values\n",
    "print(f\"\\nUnique Values (Variance Check):\")\n",
    "for feat in key_features:\n",
    "    unique_count = df[feat].nunique()\n",
    "    print(f\"  {feat:30s} {unique_count:3d} unique values\")\n",
    "    if feat == 'payload_size_kb':\n",
    "        print(f\"    Range: {df[feat].min():.1f} - {df[feat].max():.1f} KB\")\n",
    "        print(f\"    Unique sizes: {sorted(df[feat].unique())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation analysis\n",
    "print(\"\\nGenerating correlation matrix...\")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 10))\n",
    "correlation_matrix = df[FEATURE_COLUMNS + [TARGET_COLUMN]].corr()\n",
    "sns.heatmap(correlation_matrix, annot=False, cmap='coolwarm', center=0, ax=ax)\n",
    "ax.set_title('Feature Correlation Matrix', fontsize=16, fontweight='bold', pad=20)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Correlation with target\n",
    "print(f\"\\nTop 10 Features Correlated with Target:\")\n",
    "target_corr = correlation_matrix[TARGET_COLUMN].drop(TARGET_COLUMN).sort_values(ascending=False)\n",
    "print(target_corr.head(10))\n",
    "\n",
    "print(\"\\n‚úÖ EDA complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Model Training\n",
    "\n",
    "Training 4 different models:\n",
    "1. **Random Forest** - Baseline, interpretable\n",
    "2. **XGBoost** - Expected best performance\n",
    "3. **LightGBM** - Fast, efficient\n",
    "4. **Neural Network** - Comparison\n",
    "\n",
    "**Target:** 75-90% accuracy (NOT 100%!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for training\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PREPARING DATA FOR TRAINING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "X = df[FEATURE_COLUMNS].copy()\n",
    "y = df[TARGET_COLUMN].copy()\n",
    "\n",
    "print(f\"\\nFeatures (X): {X.shape}\")\n",
    "print(f\"Target (y):   {y.shape}\")\n",
    "print(f\"\\nClass distribution in y:\")\n",
    "print(y.value_counts())\n",
    "\n",
    "# Check for missing values\n",
    "missing = X.isnull().sum()\n",
    "if missing.sum() > 0:\n",
    "    print(f\"\\n‚ö†Ô∏è  Missing values detected:\")\n",
    "    print(missing[missing > 0])\n",
    "else:\n",
    "    print(f\"\\n‚úÖ No missing values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-test split (60-20-20)\n",
    "print(\"\\nSplitting data: 60% train, 20% validation, 20% test\")\n",
    "\n",
    "# First split: 60% train, 40% temp\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X, y, test_size=0.4, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Second split: 20% validation, 20% test\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp\n",
    ")\n",
    "\n",
    "print(f\"\\nTrain set:      {X_train.shape[0]:,} samples ({X_train.shape[0]/len(X)*100:.1f}%)\")\n",
    "print(f\"Validation set: {X_val.shape[0]:,} samples ({X_val.shape[0]/len(X)*100:.1f}%)\")\n",
    "print(f\"Test set:       {X_test.shape[0]:,} samples ({X_test.shape[0]/len(X)*100:.1f}%)\")\n",
    "\n",
    "# Verify stratification\n",
    "print(f\"\\nLabel distribution:\")\n",
    "print(f\"  Train:      {y_train.mean()*100:.1f}% Lambda optimal\")\n",
    "print(f\"  Validation: {y_val.mean()*100:.1f}% Lambda optimal\")\n",
    "print(f\"  Test:       {y_test.mean()*100:.1f}% Lambda optimal\")\n",
    "\n",
    "print(\"\\n‚úÖ Data split complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define models\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DEFINING ML MODELS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "models = {\n",
    "    'Random Forest': RandomForestClassifier(\n",
    "        n_estimators=200,\n",
    "        max_depth=15,\n",
    "        min_samples_split=10,\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "        verbose=0\n",
    "    ),\n",
    "    'XGBoost': XGBClassifier(\n",
    "        n_estimators=200,\n",
    "        max_depth=8,\n",
    "        learning_rate=0.1,\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "        verbosity=0\n",
    "    ),\n",
    "    'LightGBM': LGBMClassifier(\n",
    "        n_estimators=200,\n",
    "        max_depth=10,\n",
    "        learning_rate=0.1,\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "        verbose=-1\n",
    "    ),\n",
    "    'Neural Network': MLPClassifier(\n",
    "        hidden_layer_sizes=(64, 32, 16),\n",
    "        activation='relu',\n",
    "        solver='adam',\n",
    "        max_iter=500,\n",
    "        random_state=42,\n",
    "        verbose=False,\n",
    "        early_stopping=True,\n",
    "        validation_fraction=0.1\n",
    "    )\n",
    "}\n",
    "\n",
    "print(f\"\\n{len(models)} models defined:\")\n",
    "for name, model in models.items():\n",
    "    print(f\"  ‚úì {name}\")\n",
    "\n",
    "print(\"\\n‚úÖ Models ready for training!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train all models\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TRAINING MODELS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "results = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Training: {name}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Train\n",
    "    print(f\"Fitting model...\")\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Predictions\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    y_val_pred = model.predict(X_val)\n",
    "    y_test_pred = model.predict(X_test)\n",
    "    \n",
    "    # Probabilities (for ROC-AUC)\n",
    "    y_train_proba = model.predict_proba(X_train)[:, 1]\n",
    "    y_val_proba = model.predict_proba(X_val)[:, 1]\n",
    "    y_test_proba = model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    # Metrics\n",
    "    train_acc = accuracy_score(y_train, y_train_pred)\n",
    "    val_acc = accuracy_score(y_val, y_val_pred)\n",
    "    test_acc = accuracy_score(y_test, y_test_pred)\n",
    "    \n",
    "    train_roc_auc = roc_auc_score(y_train, y_train_proba)\n",
    "    val_roc_auc = roc_auc_score(y_val, y_val_proba)\n",
    "    test_roc_auc = roc_auc_score(y_test, y_test_proba)\n",
    "    \n",
    "    precision = precision_score(y_test, y_test_pred)\n",
    "    recall = recall_score(y_test, y_test_pred)\n",
    "    f1 = f1_score(y_test, y_test_pred)\n",
    "    \n",
    "    # Store results\n",
    "    results[name] = {\n",
    "        'model': model,\n",
    "        'train_acc': train_acc,\n",
    "        'val_acc': val_acc,\n",
    "        'test_acc': test_acc,\n",
    "        'train_roc_auc': train_roc_auc,\n",
    "        'val_roc_auc': val_roc_auc,\n",
    "        'test_roc_auc': test_roc_auc,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'train_test_gap': train_acc - test_acc,\n",
    "        'y_test_pred': y_test_pred,\n",
    "        'y_test_proba': y_test_proba\n",
    "    }\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"\\nüìä Results:\")\n",
    "    print(f\"  Train Accuracy:      {train_acc:.4f} ({train_acc*100:.2f}%)\")\n",
    "    print(f\"  Validation Accuracy: {val_acc:.4f} ({val_acc*100:.2f}%)\")\n",
    "    print(f\"  Test Accuracy:       {test_acc:.4f} ({test_acc*100:.2f}%)\")\n",
    "    print(f\"  Train-Test Gap:      {train_acc - test_acc:.4f}\")\n",
    "    print(f\"\\n  Test Precision:      {precision:.4f}\")\n",
    "    print(f\"  Test Recall:         {recall:.4f}\")\n",
    "    print(f\"  Test F1-Score:       {f1:.4f}\")\n",
    "    print(f\"  Test ROC-AUC:        {test_roc_auc:.4f}\")\n",
    "    \n",
    "    # Validation check\n",
    "    if test_acc > 0.95:\n",
    "        print(f\"\\n  ‚ö†Ô∏è  WARNING: Accuracy too high ({test_acc*100:.1f}%) - may still be memorizing!\")\n",
    "    elif test_acc < 0.70:\n",
    "        print(f\"\\n  ‚ö†Ô∏è  WARNING: Accuracy too low ({test_acc*100:.1f}%) - data quality issue?\")\n",
    "    elif 0.75 <= test_acc <= 0.90:\n",
    "        print(f\"\\n  ‚úÖ EXCELLENT: Accuracy in target range ({test_acc*100:.1f}%)\")\n",
    "    else:\n",
    "        print(f\"\\n  ‚úì Acceptable accuracy ({test_acc*100:.1f}%)\")\n",
    "    \n",
    "    if train_acc - test_acc < 0.05:\n",
    "        print(f\"  ‚úÖ GOOD: Low overfitting (gap: {(train_acc - test_acc)*100:.2f}%)\")\n",
    "    else:\n",
    "        print(f\"  ‚ö†Ô∏è  WARNING: Overfitting detected (gap: {(train_acc - test_acc)*100:.2f}%)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚úÖ ALL MODELS TRAINED!\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Model Evaluation & Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparison table\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MODEL COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Model': list(results.keys()),\n",
    "    'Train Acc': [r['train_acc'] for r in results.values()],\n",
    "    'Val Acc': [r['val_acc'] for r in results.values()],\n",
    "    'Test Acc': [r['test_acc'] for r in results.values()],\n",
    "    'Precision': [r['precision'] for r in results.values()],\n",
    "    'Recall': [r['recall'] for r in results.values()],\n",
    "    'F1-Score': [r['f1'] for r in results.values()],\n",
    "    'ROC-AUC': [r['test_roc_auc'] for r in results.values()],\n",
    "    'Train-Test Gap': [r['train_test_gap'] for r in results.values()]\n",
    "})\n",
    "\n",
    "# Sort by test accuracy\n",
    "comparison_df = comparison_df.sort_values('Test Acc', ascending=False).reset_index(drop=True)\n",
    "\n",
    "print(\"\\n\")\n",
    "print(comparison_df.to_string(index=False))\n",
    "\n",
    "# Best model\n",
    "best_model_name = comparison_df.iloc[0]['Model']\n",
    "best_test_acc = comparison_df.iloc[0]['Test Acc']\n",
    "\n",
    "print(f\"\\nüèÜ Best Model: {best_model_name} (Test Accuracy: {best_test_acc:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization: Model comparison\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# 1. Accuracy comparison\n",
    "ax1 = axes[0, 0]\n",
    "x = np.arange(len(comparison_df))\n",
    "width = 0.25\n",
    "ax1.bar(x - width, comparison_df['Train Acc'], width, label='Train', alpha=0.8)\n",
    "ax1.bar(x, comparison_df['Val Acc'], width, label='Validation', alpha=0.8)\n",
    "ax1.bar(x + width, comparison_df['Test Acc'], width, label='Test', alpha=0.8)\n",
    "ax1.set_xlabel('Model', fontsize=12)\n",
    "ax1.set_ylabel('Accuracy', fontsize=12)\n",
    "ax1.set_title('Accuracy Comparison (Train/Val/Test)', fontsize=14, fontweight='bold')\n",
    "ax1.set_xticks(x)\n",
    "ax1.set_xticklabels(comparison_df['Model'], rotation=15, ha='right')\n",
    "ax1.legend()\n",
    "ax1.grid(axis='y', alpha=0.3)\n",
    "ax1.axhline(y=0.75, color='green', linestyle='--', alpha=0.5, label='Min target (75%)')\n",
    "ax1.axhline(y=0.90, color='orange', linestyle='--', alpha=0.5, label='Max target (90%)')\n",
    "\n",
    "# 2. Test metrics comparison\n",
    "ax2 = axes[0, 1]\n",
    "metrics_df = comparison_df[['Model', 'Precision', 'Recall', 'F1-Score']].set_index('Model')\n",
    "metrics_df.plot(kind='bar', ax=ax2, alpha=0.8)\n",
    "ax2.set_xlabel('Model', fontsize=12)\n",
    "ax2.set_ylabel('Score', fontsize=12)\n",
    "ax2.set_title('Test Metrics Comparison', fontsize=14, fontweight='bold')\n",
    "ax2.set_xticklabels(comparison_df['Model'], rotation=15, ha='right')\n",
    "ax2.legend()\n",
    "ax2.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# 3. Overfitting analysis\n",
    "ax3 = axes[1, 0]\n",
    "colors = ['green' if gap < 0.05 else 'orange' for gap in comparison_df['Train-Test Gap']]\n",
    "ax3.bar(comparison_df['Model'], comparison_df['Train-Test Gap'], color=colors, alpha=0.8)\n",
    "ax3.set_xlabel('Model', fontsize=12)\n",
    "ax3.set_ylabel('Train-Test Gap', fontsize=12)\n",
    "ax3.set_title('Overfitting Analysis (lower is better)', fontsize=14, fontweight='bold')\n",
    "ax3.set_xticklabels(comparison_df['Model'], rotation=15, ha='right')\n",
    "ax3.axhline(y=0.05, color='red', linestyle='--', linewidth=2, alpha=0.7, label='5% threshold')\n",
    "ax3.legend()\n",
    "ax3.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# 4. ROC-AUC comparison\n",
    "ax4 = axes[1, 1]\n",
    "ax4.barh(comparison_df['Model'], comparison_df['ROC-AUC'], alpha=0.8, color='purple')\n",
    "ax4.set_xlabel('ROC-AUC Score', fontsize=12)\n",
    "ax4.set_ylabel('Model', fontsize=12)\n",
    "ax4.set_title('ROC-AUC Comparison', fontsize=14, fontweight='bold')\n",
    "ax4.axvline(x=0.75, color='green', linestyle='--', alpha=0.5)\n",
    "ax4.axvline(x=0.90, color='orange', linestyle='--', alpha=0.5)\n",
    "ax4.grid(axis='x', alpha=0.3)\n",
    "for i, v in enumerate(comparison_df['ROC-AUC']):\n",
    "    ax4.text(v + 0.01, i, f'{v:.4f}', va='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úÖ Model comparison visualizations complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrices\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, (name, res) in enumerate(results.items()):\n",
    "    ax = axes[idx]\n",
    "    cm = confusion_matrix(y_test, res['y_test_pred'])\n",
    "    \n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax, cbar=False)\n",
    "    ax.set_title(f'{name}\\nTest Acc: {res[\"test_acc\"]:.4f}', fontsize=12, fontweight='bold')\n",
    "    ax.set_xlabel('Predicted', fontsize=11)\n",
    "    ax.set_ylabel('Actual', fontsize=11)\n",
    "    ax.set_xticklabels(['ECS (0)', 'Lambda (1)'])\n",
    "    ax.set_yticklabels(['ECS (0)', 'Lambda (1)'])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úÖ Confusion matrices generated!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC curves\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "for name, res in results.items():\n",
    "    fpr, tpr, _ = roc_curve(y_test, res['y_test_proba'])\n",
    "    auc = res['test_roc_auc']\n",
    "    plt.plot(fpr, tpr, label=f'{name} (AUC = {auc:.4f})', linewidth=2)\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--', label='Random Classifier', linewidth=2)\n",
    "plt.xlabel('False Positive Rate', fontsize=13)\n",
    "plt.ylabel('True Positive Rate', fontsize=13)\n",
    "plt.title('ROC Curves - All Models', fontsize=16, fontweight='bold')\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úÖ ROC curves generated!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Feature Importance Analysis\n",
    "\n",
    "Understanding which features the model uses to make decisions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance for tree-based models\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FEATURE IMPORTANCE ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "tree_models = ['Random Forest', 'XGBoost', 'LightGBM']\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "for idx, model_name in enumerate(tree_models):\n",
    "    if model_name in results:\n",
    "        model = results[model_name]['model']\n",
    "        \n",
    "        # Get feature importances\n",
    "        if hasattr(model, 'feature_importances_'):\n",
    "            importances = model.feature_importances_\n",
    "            indices = np.argsort(importances)[::-1]\n",
    "            \n",
    "            # Print top 10\n",
    "            print(f\"\\n{model_name} - Top 10 Features:\")\n",
    "            for i in range(min(10, len(indices))):\n",
    "                feat_idx = indices[i]\n",
    "                print(f\"  {i+1:2d}. {FEATURE_COLUMNS[feat_idx]:40s} {importances[feat_idx]:.6f} ({importances[feat_idx]*100:.2f}%)\")\n",
    "            \n",
    "            # Check for dominance\n",
    "            max_importance = importances[indices[0]]\n",
    "            if max_importance > 0.40:\n",
    "                print(f\"\\n  ‚ö†Ô∏è  WARNING: Top feature dominates ({max_importance*100:.1f}%) - may still be lookup table!\")\n",
    "            else:\n",
    "                print(f\"\\n  ‚úÖ GOOD: Feature importance distributed (top: {max_importance*100:.1f}%)\")\n",
    "            \n",
    "            # Plot\n",
    "            ax = axes[idx]\n",
    "            top_n = 10\n",
    "            top_indices = indices[:top_n]\n",
    "            top_features = [FEATURE_COLUMNS[i] for i in top_indices]\n",
    "            top_importances = importances[top_indices]\n",
    "            \n",
    "            ax.barh(range(top_n), top_importances, alpha=0.8)\n",
    "            ax.set_yticks(range(top_n))\n",
    "            ax.set_yticklabels(top_features, fontsize=9)\n",
    "            ax.set_xlabel('Importance', fontsize=11)\n",
    "            ax.set_title(f'{model_name}', fontsize=12, fontweight='bold')\n",
    "            ax.invert_yaxis()\n",
    "            ax.grid(axis='x', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úÖ Feature importance analysis complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. SHAP Analysis (Interpretability)\n",
    "\n",
    "Using SHAP (SHapley Additive exPlanations) to understand model predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHAP analysis for best model\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SHAP ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "best_model = results[best_model_name]['model']\n",
    "\n",
    "print(f\"\\nGenerating SHAP values for: {best_model_name}\")\n",
    "print(\"This may take a few minutes...\")\n",
    "\n",
    "# Sample data for SHAP (use subset for speed)\n",
    "shap_sample_size = min(1000, len(X_test))\n",
    "X_shap = X_test.sample(n=shap_sample_size, random_state=42)\n",
    "\n",
    "# Create explainer\n",
    "if best_model_name in ['Random Forest', 'XGBoost', 'LightGBM']:\n",
    "    explainer = shap.TreeExplainer(best_model)\n",
    "    shap_values = explainer.shap_values(X_shap)\n",
    "    \n",
    "    # For binary classification, shap_values might be a list\n",
    "    if isinstance(shap_values, list):\n",
    "        shap_values = shap_values[1]  # Use positive class\n",
    "else:\n",
    "    # For Neural Network, use KernelExplainer (slower)\n",
    "    explainer = shap.KernelExplainer(best_model.predict_proba, shap.sample(X_train, 100))\n",
    "    shap_values = explainer.shap_values(X_shap)\n",
    "    if isinstance(shap_values, list):\n",
    "        shap_values = shap_values[1]\n",
    "\n",
    "print(\"\\n‚úÖ SHAP values computed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHAP summary plot\n",
    "print(\"\\nGenerating SHAP summary plot...\")\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "shap.summary_plot(shap_values, X_shap, feature_names=FEATURE_COLUMNS, show=False)\n",
    "plt.title(f'SHAP Summary Plot - {best_model_name}', fontsize=16, fontweight='bold', pad=20)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úÖ SHAP summary plot complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHAP feature importance (bar plot)\n",
    "print(\"\\nGenerating SHAP feature importance...\")\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "shap.summary_plot(shap_values, X_shap, feature_names=FEATURE_COLUMNS, plot_type=\"bar\", show=False)\n",
    "plt.title(f'SHAP Feature Importance - {best_model_name}', fontsize=16, fontweight='bold', pad=20)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úÖ SHAP feature importance complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Model Selection & Saving for AWS Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select best model\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MODEL SELECTION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nüèÜ Selected Model: {best_model_name}\")\n",
    "print(f\"\\nüìä Performance Metrics:\")\n",
    "print(f\"  Test Accuracy:  {results[best_model_name]['test_acc']:.4f} ({results[best_model_name]['test_acc']*100:.2f}%)\")\n",
    "print(f\"  Precision:      {results[best_model_name]['precision']:.4f}\")\n",
    "print(f\"  Recall:         {results[best_model_name]['recall']:.4f}\")\n",
    "print(f\"  F1-Score:       {results[best_model_name]['f1']:.4f}\")\n",
    "print(f\"  ROC-AUC:        {results[best_model_name]['test_roc_auc']:.4f}\")\n",
    "print(f\"  Train-Test Gap: {results[best_model_name]['train_test_gap']:.4f}\")\n",
    "\n",
    "# Model size\n",
    "import sys\n",
    "model_size_bytes = sys.getsizeof(best_model)\n",
    "model_size_mb = model_size_bytes / (1024 * 1024)\n",
    "print(f\"\\n  Estimated Size: {model_size_mb:.2f} MB\")\n",
    "\n",
    "# Validation\n",
    "test_acc = results[best_model_name]['test_acc']\n",
    "if 0.75 <= test_acc <= 0.90:\n",
    "    print(f\"\\n‚úÖ SUCCESS: Accuracy in target range (75-90%)\")\n",
    "    print(f\"   Model learned meaningful patterns, not memorization!\")\n",
    "elif test_acc > 0.95:\n",
    "    print(f\"\\n‚ö†Ô∏è  WARNING: Accuracy too high ({test_acc*100:.1f}%)\")\n",
    "    print(f\"   Model may still be memorizing workload_type ‚Üí label\")\n",
    "else:\n",
    "    print(f\"\\n‚úì Acceptable accuracy ({test_acc*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model and metadata\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SAVING MODEL FOR DEPLOYMENT\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Create models directory\n",
    "import os\n",
    "os.makedirs('models', exist_ok=True)\n",
    "\n",
    "# Save model\n",
    "model_filename = 'models/variance_model_best.pkl'\n",
    "joblib.dump(best_model, model_filename)\n",
    "print(f\"\\n‚úÖ Model saved: {model_filename}\")\n",
    "\n",
    "# Save feature columns\n",
    "features_filename = 'models/feature_columns.json'\n",
    "with open(features_filename, 'w') as f:\n",
    "    json.dump(FEATURE_COLUMNS, f, indent=2)\n",
    "print(f\"‚úÖ Features saved: {features_filename}\")\n",
    "\n",
    "# Save metadata\n",
    "metadata = {\n",
    "    'model_type': best_model_name,\n",
    "    'training_date': datetime.now().isoformat(),\n",
    "    'training_samples': len(X_train),\n",
    "    'validation_samples': len(X_val),\n",
    "    'test_samples': len(X_test),\n",
    "    'num_features': len(FEATURE_COLUMNS),\n",
    "    'feature_columns': FEATURE_COLUMNS,\n",
    "    'performance': {\n",
    "        'train_accuracy': float(results[best_model_name]['train_acc']),\n",
    "        'val_accuracy': float(results[best_model_name]['val_acc']),\n",
    "        'test_accuracy': float(results[best_model_name]['test_acc']),\n",
    "        'precision': float(results[best_model_name]['precision']),\n",
    "        'recall': float(results[best_model_name]['recall']),\n",
    "        'f1_score': float(results[best_model_name]['f1']),\n",
    "        'roc_auc': float(results[best_model_name]['test_roc_auc']),\n",
    "        'train_test_gap': float(results[best_model_name]['train_test_gap'])\n",
    "    },\n",
    "    'model_config': str(best_model.get_params()) if hasattr(best_model, 'get_params') else 'N/A',\n",
    "    'target_variable': TARGET_COLUMN,\n",
    "    'class_labels': {'0': 'ECS', '1': 'Lambda'}\n",
    "}\n",
    "\n",
    "metadata_filename = 'models/model_metadata.json'\n",
    "with open(metadata_filename, 'w') as f:\n",
    "    json.dump(metadata, f, indent=2)\n",
    "print(f\"‚úÖ Metadata saved: {metadata_filename}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MODEL DEPLOYMENT PACKAGE READY\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nüì¶ Files for AWS Lambda deployment:\")\n",
    "print(f\"  1. {model_filename}\")\n",
    "print(f\"  2. {features_filename}\")\n",
    "print(f\"  3. {metadata_filename}\")\n",
    "print(f\"\\nüí° Next: Download these files and use with inference script\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download files from Colab (if running on Colab)\n",
    "print(\"\\nüì• Downloading model files...\\n\")\n",
    "\n",
    "try:\n",
    "    from google.colab import files\n",
    "    \n",
    "    files.download(model_filename)\n",
    "    files.download(features_filename)\n",
    "    files.download(metadata_filename)\n",
    "    \n",
    "    print(\"\\n‚úÖ Files downloaded successfully!\")\n",
    "except:\n",
    "    print(\"Not running on Colab - files saved locally\")\n",
    "    print(f\"Files location: {os.path.abspath('models/')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Inference Testing\n",
    "\n",
    "Test the saved model to ensure it can make predictions correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test model loading and inference\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"INFERENCE TESTING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Load saved model\n",
    "print(\"\\nLoading saved model...\")\n",
    "loaded_model = joblib.load(model_filename)\n",
    "print(f\"‚úÖ Model loaded: {type(loaded_model).__name__}\")\n",
    "\n",
    "# Load feature columns\n",
    "with open(features_filename, 'r') as f:\n",
    "    loaded_features = json.load(f)\n",
    "print(f\"‚úÖ Features loaded: {len(loaded_features)} features\")\n",
    "\n",
    "# Test prediction on sample data\n",
    "print(\"\\nTesting predictions on 5 random samples...\\n\")\n",
    "\n",
    "test_samples = X_test.sample(n=5, random_state=42)\n",
    "test_labels = y_test.loc[test_samples.index]\n",
    "\n",
    "for idx, (sample_idx, sample_row) in enumerate(test_samples.iterrows(), 1):\n",
    "    # Make prediction\n",
    "    sample_array = sample_row.values.reshape(1, -1)\n",
    "    prediction = loaded_model.predict(sample_array)[0]\n",
    "    probability = loaded_model.predict_proba(sample_array)[0]\n",
    "    actual = test_labels.loc[sample_idx]\n",
    "    \n",
    "    # Get sample info\n",
    "    workload = df.loc[sample_idx, 'workload_type']\n",
    "    payload = sample_row['payload_size_kb']\n",
    "    time_window = df.loc[sample_idx, 'time_window']\n",
    "    load_pattern = df.loc[sample_idx, 'load_pattern']\n",
    "    \n",
    "    print(f\"Sample {idx}:\")\n",
    "    print(f\"  Context: {workload}, {payload:.0f} KB, {time_window}, {load_pattern}\")\n",
    "    print(f\"  Prediction: {'Lambda' if prediction == 1 else 'ECS'}\")\n",
    "    print(f\"  Confidence: {max(probability)*100:.1f}% (Lambda: {probability[1]*100:.1f}%, ECS: {probability[0]*100:.1f}%)\")\n",
    "    print(f\"  Actual:     {'Lambda' if actual == 1 else 'ECS'}\")\n",
    "    print(f\"  Correct:    {'‚úÖ' if prediction == actual else '‚ùå'}\")\n",
    "    print()\n",
    "\n",
    "print(\"‚úÖ Inference testing complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference time measurement\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"INFERENCE LATENCY MEASUREMENT\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "import time\n",
    "\n",
    "# Test inference speed\n",
    "num_predictions = 1000\n",
    "test_batch = X_test.sample(n=num_predictions, random_state=42)\n",
    "\n",
    "print(f\"\\nMeasuring inference time for {num_predictions} predictions...\")\n",
    "\n",
    "start_time = time.time()\n",
    "predictions = loaded_model.predict(test_batch)\n",
    "end_time = time.time()\n",
    "\n",
    "total_time = end_time - start_time\n",
    "avg_time_ms = (total_time / num_predictions) * 1000\n",
    "\n",
    "print(f\"\\nüìä Inference Performance:\")\n",
    "print(f\"  Total time:         {total_time:.4f} seconds\")\n",
    "print(f\"  Average per request: {avg_time_ms:.4f} ms\")\n",
    "print(f\"  Throughput:         {num_predictions/total_time:.0f} predictions/second\")\n",
    "\n",
    "if avg_time_ms < 5:\n",
    "    print(f\"\\n‚úÖ EXCELLENT: Inference latency < 5ms (suitable for real-time)\")\n",
    "elif avg_time_ms < 10:\n",
    "    print(f\"\\n‚úÖ GOOD: Inference latency < 10ms (acceptable)\")\n",
    "else:\n",
    "    print(f\"\\n‚ö†Ô∏è  WARNING: Inference latency > 10ms (may need optimization)\")\n",
    "\n",
    "print(\"\\n‚úÖ Latency measurement complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 9. Summary & Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TRAINING SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nüìä Dataset:\")\n",
    "print(f\"  Total samples:      {len(df):,}\")\n",
    "print(f\"  Training samples:   {len(X_train):,}\")\n",
    "print(f\"  Validation samples: {len(X_val):,}\")\n",
    "print(f\"  Test samples:       {len(X_test):,}\")\n",
    "print(f\"  Features:           {len(FEATURE_COLUMNS)}\")\n",
    "\n",
    "print(f\"\\nüèÜ Best Model: {best_model_name}\")\n",
    "print(f\"  Test Accuracy:      {results[best_model_name]['test_acc']*100:.2f}%\")\n",
    "print(f\"  Precision:          {results[best_model_name]['precision']:.4f}\")\n",
    "print(f\"  Recall:             {results[best_model_name]['recall']:.4f}\")\n",
    "print(f\"  F1-Score:           {results[best_model_name]['f1']:.4f}\")\n",
    "print(f\"  ROC-AUC:            {results[best_model_name]['test_roc_auc']:.4f}\")\n",
    "print(f\"  Overfitting Gap:    {results[best_model_name]['train_test_gap']*100:.2f}%\")\n",
    "\n",
    "print(f\"\\nüì¶ Deployment Files:\")\n",
    "print(f\"  Model:              {model_filename}\")\n",
    "print(f\"  Features:           {features_filename}\")\n",
    "print(f\"  Metadata:           {metadata_filename}\")\n",
    "\n",
    "print(f\"\\n‚ö° Inference Performance:\")\n",
    "print(f\"  Average latency:    {avg_time_ms:.4f} ms\")\n",
    "print(f\"  Throughput:         {num_predictions/total_time:.0f} pred/sec\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚úÖ TRAINING COMPLETE - MODEL READY FOR AWS DEPLOYMENT\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nüöÄ Next Steps:\")\n",
    "print(f\"  1. Download model files (variance_model_best.pkl, feature_columns.json, model_metadata.json)\")\n",
    "print(f\"  2. Use the AWS Lambda inference script provided\")\n",
    "print(f\"  3. Deploy to AWS Lambda for real-time routing\")\n",
    "print(f\"  4. Run comparative evaluation (Lambda-only vs ECS-only vs ML-hybrid)\")\n",
    "print(f\"  5. Measure actual cost and latency savings\")\n",
    "\n",
    "print(f\"\\nüí° Key Achievement:\")\n",
    "print(f\"  Model accuracy {results[best_model_name]['test_acc']*100:.1f}% indicates real learning!\")\n",
    "if 0.75 <= results[best_model_name]['test_acc'] <= 0.90:\n",
    "    print(f\"  ‚úÖ SUCCESS: Not a lookup table - model learned meaningful decision boundaries!\")\n",
    "else:\n",
    "    print(f\"  ‚ö†Ô∏è  Review feature importance to ensure no single feature dominates\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
